{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wehco = pd.read_csv(\"C:/Users/adity/Downloads/SRC Project/LDA/content_lvl2_0802.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean headline column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wehco[\"headline\"] = content_wehco[\"headline\"].astype('str')\n",
    "content_wehco[\"headline\"] = content_wehco[\"headline\"].str.encode(\"ascii\", \"ignore\").str.decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating bigrams / trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('central arkansas','centralarkansas')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('little rock','littlerock')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('donald trump','donaldtrump')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('covid 19','coronavirus')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('covid-19','coronavirus')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('covid','coronavirus')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('dear abby','dearabby')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('pages from the past','pagesfromthepast')\n",
    "\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('new york','ny')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('syracuse university','su')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('president trump','trump')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('donald trump','trump')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('new year','new year')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"trump's\",'trump')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('president obama','obama')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('dear abby','dearabby')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('pm buzz','pmbuzz')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('viral video','viralvideo')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('star wars','starwars')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('white house','whitehouse')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('high school','highschool')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"women's\",'woman')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"men's\",'men')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"buffalo-bill\",'buffalobill')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"boeheims\",'boeheim')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"boeheim's\",'boeheim')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"jim boeheim\",'boeheim')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"information\",'info')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace(\"notre dame\",'notredame')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.lower().str.replace('syracuse','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new column for level 1 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wehco[\"nlp_level_1\"] = \"Others\"\n",
    "\n",
    "matches = [\"BRIDGE\", \"CROSSWORD\", \"DOKU\", \"SLEUTH\", \"DOKO\", \"JUM\", \"TOSHIKI\", \"TORI\"]\n",
    "\n",
    "for i in range(0, len(content_wehco)):\n",
    "    if (re.search(\"Obituaries\", str(content_wehco.headline[i]), re.IGNORECASE)):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Obituaries\"\n",
    "    elif (re.search(\"Obituary\", str(content_wehco.headline[i]), re.IGNORECASE)):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Obituaries\"\n",
    "    elif (\"Opinion\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Opinion\"\n",
    "    elif (\"Weather\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Weather\"\n",
    "    elif (\"Promotions\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Promotions\"\n",
    "    elif any(x in str(content_wehco.headline[i]).upper() for x in matches):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Games\"\n",
    "    elif (\"Coronavirus\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Coronavirus\"\n",
    "    elif (\"Business\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Business\"\n",
    "    elif (\"Editorial\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Editorial\"\n",
    "    elif (\"Crime\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Crime\"\n",
    "    elif (\"Entertainment\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Entertainment\"\n",
    "    elif (\"Sports\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"Sports\"\n",
    "    elif (\"News\" in str(content_wehco.cmsCategories[i])):\n",
    "        content_wehco[\"nlp_level_1\"][i] = \"News\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ML to classify \"others\" as level 1 tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', \"%\", 'm', 'p', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    " 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"about\", \"across\", \"after\", \"all\", \n",
    " \"also\", \"an\", \"and\", \"another\", \"added\", \"any\", \"are\", \"as\", \"at\", \"basically\", \"be\", \"because\", 'become', \"been\", \"before\", \n",
    " \"being\", \"between\", \"both\", \"but\", \"by\", \"came\", \"can\", \"come\", \"could\", \"did\", \"do\", \"does\", \"each\", \"else\", \"every\",\n",
    " \"either\", \"especially\", \"for\", \"from\", \"get\", \"given\", \"gets\", 'give', 'gives', \"got\", \"goes\", \"had\", \"has\", \"have\", \"he\", \n",
    " \"her\", \"here\", \"him\", \"himself\", \"his\", \"how\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"lands\", \"like\", \"make\", \n",
    " \"making\", \"made\", \"many\", \"may\", \"me\", \"might\", \"more\", \"most\", \"much\", \"must\", \"my\", \"never\", \"provide\", \"provides\", \n",
    " \"perhaps\", \"no\", \"now\", \"of\", \"on\", \"only\", \"or\", \"other\", \"our\", \"out\", \"over\", \"re\", \"said\", \"same\", \"see\", \"should\", \n",
    " \"since\", \"so\", \"some\", \"still\", \"such\", \"seeing\", \"see\", \"take\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \n",
    " \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"up\", \"use\", \"using\", \"used\", \"underway\", \"very\", \"want\", \n",
    " \"was\", \"way\", \"we\", \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"whilst\", \"who\", \"will\", \"with\", \"would\", \n",
    " \"you\", \"your\", 'etc', 'via', 'eg', 'news', \"'s\", \"april\", \"march\", \"june\", \"july\", \"august\", \"february\", \"january\", \"december\", \n",
    " \"september\", \"october\", \"dec\", \"oct\", \"nov\", \"today's\", \"'the'\", \"sept\", \"feb\", \"jan\", \"amp\", \"say\", \"'the\", \"friday's\", \"friday\",\n",
    " \"saturday\", \"saturday's\", \"sunday\", \"sunday's\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(extra_stop_words)\n",
    "\n",
    "content_wehco['headline'] = content_wehco['headline'].str.replace('obituaries','obits')\n",
    "content_wehco['headline'] = content_wehco['headline'].str.replace('obituary','obits')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [wn.lemmatize(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating subset without \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Others           50798\n",
       "News             49448\n",
       "Sports           24360\n",
       "Entertainment    18907\n",
       "Crime            10658\n",
       "Business          9241\n",
       "Coronavirus       7355\n",
       "Editorial         7248\n",
       "Obituaries        6256\n",
       "Games             5965\n",
       "Promotions        4160\n",
       "Opinion           1606\n",
       "Weather            928\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_subset_without_others = content_wehco[content_wehco[\"nlp_level_1\"] != \"Others\"]\n",
    "content_wehco[\"pred\"] = content_wehco[\"nlp_level_1\"]\n",
    "content_wehco[\"conf\"] = 0\n",
    "content_wehco[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News             49448\n",
       "Sports           24360\n",
       "Entertainment    18907\n",
       "Crime            10658\n",
       "Business          9241\n",
       "Coronavirus       7355\n",
       "Editorial         7248\n",
       "Obituaries        6256\n",
       "Games             5965\n",
       "Promotions        4160\n",
       "Opinion           1606\n",
       "Weather            928\n",
       "Name: nlp_level_1, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_subset_without_others[\"nlp_level_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_subset_without_others['headline'], content_subset_without_others['nlp_level_1'], random_state = 0)\n",
    "count_vect = CountVectorizer(min_df = 30, stop_words = stop_words, tokenizer = tokenize, analyzer = 'word', max_features = 50000)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf = TfidfTransformer(sublinear_tf = True, norm = 'l2', smooth_idf = False)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf = CalibratedClassifierCV(LinearSVC()).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News             72781\n",
       "Sports           33871\n",
       "Entertainment    25110\n",
       "Crime            19092\n",
       "Business         10791\n",
       "Coronavirus       8923\n",
       "Obituaries        6261\n",
       "Promotions        6211\n",
       "Games             5750\n",
       "Editorial         5528\n",
       "Weather           1679\n",
       "Opinion            933\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_wehco[\"pred\"] = clf.predict(count_vect.transform(content_wehco[\"headline\"]))\n",
    "content_wehco[\"conf\"] = (clf.predict_proba(count_vect.transform(content_wehco[\"headline\"]))).max(axis = 1)\n",
    "content_wehco[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wehco[\"final_pred\"] = np.where((content_wehco[\"nlp_level_1\"] == \"Others\") & (content_wehco[\"conf\"] >= 0.6), content_wehco[\"pred\"], content_wehco[\"nlp_level_1\"])\n",
    "content_wehco['counts'] = content_wehco['final_pred'].map(content_wehco['final_pred'].value_counts())\n",
    "content_wehco.to_csv(\"content_wehco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News             57536\n",
       "Sports           29661\n",
       "Others           27243\n",
       "Entertainment    23549\n",
       "Crime            11749\n",
       "Business         10430\n",
       "Coronavirus       9353\n",
       "Editorial         7522\n",
       "Obituaries        6257\n",
       "Games             5975\n",
       "Promotions        4944\n",
       "Opinion           1674\n",
       "Weather           1037\n",
       "Name: final_pred, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_wehco[\"final_pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting each of the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_subset_obit = content_wehco[content_wehco[\"final_pred\"] == \"Obituaries\"]\n",
    "content_subset_weather = content_wehco[content_wehco[\"final_pred\"] == \"Weather\"]\n",
    "content_subset_games = content_wehco[content_wehco[\"final_pred\"] == \"Games\"] # political\n",
    "content_subset_business = content_wehco[content_wehco[\"final_pred\"] == \"Business\"]\n",
    "content_subset_promotions = content_wehco[content_wehco[\"final_pred\"] == \"Promotions\"] # landc\n",
    "content_subset_opinion = content_wehco[content_wehco[\"final_pred\"] == \"Opinion\"]\n",
    "content_subset_crime = content_wehco[content_wehco[\"final_pred\"] == \"Crime\"]\n",
    "content_subset_editorial = content_wehco[content_wehco[\"final_pred\"] == \"Editorial\"] # local news\n",
    "content_subset_entertainment = content_wehco[content_wehco[\"final_pred\"] == \"Entertainment\"]\n",
    "content_subset_sports = content_wehco[content_wehco[\"final_pred\"] == \"Sports\"]\n",
    "content_subset_news = content_wehco[content_wehco[\"final_pred\"] == \"News\"]\n",
    "content_subset_others = content_wehco[content_wehco[\"final_pred\"] == \"Others\"]\n",
    "content_subset_covid = content_wehco[content_wehco[\"final_pred\"] == \"Coronavirus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to try\n",
    "1. K Means\n",
    "2. Guided LDA\n",
    "3. LDA\n",
    "4. NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : obit, lee, william, jr, robert, jean, john, charles, david\n",
      "Cluster 1 : james, obit, jr, edward, sr, lee, thomas, david, williams\n",
      "Cluster 2 : ann, obit, patricia, mary, shirley, carolyn, betty, margaret, barbara\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_obit[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 3\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : arkansas, state, flooding, rain, flood, snow, river, wind, levee\n",
      "Cluster 1 : weather, severe, arkansas, risk, forecaster, possible, state, storm, winter\n",
      "Cluster 2 : tornado, arkansas, storm, damage, state, hit, weather, strong, wind\n",
      "Cluster 3 : storm, power, south, state, arkansas, hit, wind, severe, county\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_weather[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : sudoku, killer, conceptis, pro, littlerock, crossword, commuter, historic, kid\n",
      "Cluster 1 : jumble, daily, crossword, kid, commuter, tv, puzzle, sudoku, year\n",
      "Cluster 2 : crossword, universal, premier, tv, observer, puzzle, year, jump, jumble\n",
      "Cluster 3 : editorial, page, guide, new, time, kid, arkansas, year, set\n",
      "Cluster 4 : bridge, time, la, historic, crossword, futoshiki, hitori, wordsleuth, ace\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_games[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : stock, bankruptcy, watch, arkansas, index, real, estate, building, transaction\n",
      "Cluster 1 : business, people, northwest, arkansas, nw, company, report, firm, walmart\n",
      "Cluster 2 : brief, business, northwest, arkansas, open, year, fed, farm, farmer\n",
      "Cluster 3 : pay, retailer, settle, walmart, company, lawsuit, store, bank, apple\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_business[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : new, cabot, searcy, year, community, area, county, spring, arkansas\n",
      "Cluster 1 : conway, ann, man, woman, artist, school, new, project, work\n",
      "Cluster 2 : center, senior, community, white, director, work, child, bryant, present\n",
      "Cluster 3 : russellville, new, young, mayor, park, city, year, chief, highschool\n",
      "Cluster 4 : coach, state, title, win, football, new, lead, team, longtime\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_promotions[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : masterson, online, mike, read, opinion, letter, writer, rex, nwa\n",
      "Cluster 1 : rex, nelson, john, nwa, editorial, brummett, guest, writer, opinion\n",
      "Cluster 2 : opinion, editorial, writer, rex, read, online, nwa, nelson, mike\n",
      "Cluster 3 : letter, nwa, writer, rex, read, opinion, online, nelson, mike\n",
      "Cluster 4 : brummett, online, read, john, opinion, writer, rex, nwa, nelson\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_opinion[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : littlerock, police, man, north, shot, shooting, woman, home, robbed\n",
      "Cluster 1 : arkansas, man, police, shooting, suspect, burglary, charge, accused, case\n",
      "Cluster 2 : arrested, man, arkansas, police, littlerock, shooting, suspect, charge, woman\n",
      "Cluster 3 : beat, police, man, arkansan, sentenced, threat, lead, assault, outside\n",
      "Cluster 4 : guilty, pleads, arkansas, man, innocent, ex, case, year, woman\n",
      "Cluster 5 : year, old, man, arkansas, littlerock, term, girl, sentenced, prison\n",
      "Cluster 6 : charged, man, arkansas, murder, woman, shooting, death, slaying, killing\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_crime[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 7\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : dearabby, arkansas, home, talk, calendar, new, restaurant, space, movie\n",
      "Cluster 1 : horoscope, holiday, opinion, flavor, foundation, forward, food, folk, focus\n",
      "Cluster 2 : recipe, cake, alley, idea, food, favorite, dish, chicken, flavor\n",
      "Cluster 3 : wedding, plan, dearabby, william, dr, thomas, elizabeth, anniversary, new\n",
      "Cluster 4 : hint, helpful, opinion, gizmo, free, foundation, forward, food, folk\n",
      "Cluster 5 : super, quiz, word, title, literature, science, film, letter, country\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_entertainment[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 6\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : cartoon, deering, john, trump, impeachment, state, democrat, new, don't\n",
      "Cluster 1 : burner, best, martin, seller, opinion, time, good, life, philip\n",
      "Cluster 2 : nelson, rex, arkansas, opinion, state, big, time, long, holiday\n",
      "Cluster 3 : columnist, impeachment, trump, bernie, democrat, new, day, save, wa\n",
      "Cluster 4 : letter, editor, opinion, john, year, ha, democrat, don't, free\n",
      "Cluster 5 : masterson, mike, opinion, change, good, matter, thing, bad, don't\n",
      "Cluster 6 : brummett, john, it's, opinion, new, joe, question, man, matter\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_editorial[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 7\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : sport, brief, state, roundup, arkansas, outdoors, honor, tech, earn\n",
      "Cluster 1 : hog, ex, sec, recruiting, texas, visit, offer, commits, future\n",
      "Cluster 2 : ua, game, win, razorback, coach, state, team, hall, title\n",
      "Cluster 3 : arkansas, postcard, past, live, visit, state, texas, game, recruiting\n",
      "Cluster 4 : post, bass, loss, ua, score, favorite, bad, looking, defense\n",
      "Cluster 5 : football, highschool, prep, basketball, roundup, schedule, coach, college, week\n",
      "Cluster 6 : season, end, start, hog, arkansas, open, nfl, final, opener\n",
      "Cluster 7 : wire, green, finish, late, victory, win, yurachek, ground, greenwood\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_sports[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : trump, county, state, littlerock, new, school, notebook, plan, judge\n",
      "Cluster 1 : ua, notebook, gift, student, trustee, campus, scholarship, tuition, aid\n",
      "Cluster 2 : brief, nation, world, washington, state, capitol, file, law, legislative\n",
      "Cluster 3 : arkansas, northwest, court, roundup, supreme, arrest, appeal, state, achiever\n",
      "Cluster 4 : police, man, crash, killed, littlerock, arkansas, dy, officer, year\n",
      "Cluster 5 : day, face, closing, memorial, veteran, labor, long, election, year\n",
      "Cluster 6 : lr, city, board, school, littlerock, spa, plan, district, ok\n",
      "Cluster 7 : record, daily, filing, crawford, county, set, jefferson, marriage, meeting\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_news[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : state, new, littlerock, arkansas, best, online, year, set, case\n",
      "Cluster 1 : arkansas, week, state, case, county, test, south, economic, men\n",
      "Cluster 2 : tv, talk, littlerock, set, leave, howard, idea, watch, people\n",
      "Cluster 3 : weather, road, draw, watch, travel, arkansas, threat, california, step\n",
      "Cluster 4 : benton, ann, james, robert, faye, elizabeth, mary, sr, william\n",
      "Cluster 5 : bluff, pine, mr, johnson, sr, jr, ann, lee, george\n",
      "Cluster 6 : home, mountain, stay, week's, expensive, sold, littlerock, cost, state\n",
      "Cluster 7 : service, memorial, helena, arkansas, la, fort, firm, library, department\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_others[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : brief, nation, world, washington, state, sport, county, governor, trump\n",
      "Cluster 1 : state, case, coronavirus, site, screening, offer, virus, death, rise\n",
      "Cluster 2 : coronavirus, arkansas, test, positive, case, la, update, dy, patient\n",
      "Cluster 3 : face, pandemic, trump, arkansas, aid, reopening, stock, plan, mask\n",
      "Cluster 4 : virus, case, death, trump, hit, arkansas, offer, spread, aid\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_covid[['headline']]\n",
    "document = list(content_subset_headline[\"headline\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify Penn tags to n (NOUN), v (VERB), a (ADJECTIVE) or r (ADVERB)\n",
    "def simplify(penn_tag):\n",
    "    pre = penn_tag[0]\n",
    "    if (pre == 'J'):\n",
    "        return 'a'\n",
    "    elif (pre == 'R'):\n",
    "        return 'r'\n",
    "    elif (pre == 'V'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def preprocess(text):\n",
    "    toks = gensim.utils.simple_preprocess(str(text), deacc = True)\n",
    "    wn = WordNetLemmatizer()\n",
    "    return [wn.lemmatize(tok, simplify(pos)) for tok, pos in nltk.pos_tag(toks) if tok not in stop_words]\n",
    "\n",
    "def test_eta_lda(eta, dictionary, ntopics, print_topics = True, print_dist = True):\n",
    "    np.random.seed(42) # set the random seed for repeatability\n",
    "    bow = [dictionary.doc2bow(line) for line in corp] # get the bow-format lines with the set dictionary\n",
    "    with (np.errstate(divide = 'ignore')):  # ignore divide-by-zero warnings\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus = bow, id2word = dictionary, num_topics = ntopics,\n",
    "            random_state = 42, eta = eta, alpha = 'auto')\n",
    "    # visuzlize the model term topics\n",
    "    print('Perplexity: {:.2f}'.format(model.log_perplexity(bow)))\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        for topic in range(ntopics):\n",
    "            print('Topic {}: {}'.format(topic, [dictionary[w] for w, p in model.get_topic_terms(topic, topn = 10)]))\n",
    "#     if print_dist:\n",
    "#         # display the topic probabilities for each document\n",
    "#         for line, bag in zip(txt, bow):\n",
    "#             doc_topics = ['({}, {:.1%})'.format(topic, prob) for topic, prob in model.get_document_topics(bag)]\n",
    "#             print('{} {}'.format(line, doc_topics))\n",
    "    return model\n",
    "\n",
    "def test_eta_nmf(eta, dictionary, train_headlines_sentences, ntopics, print_topics = True, print_dist = True):\n",
    "    vectorizer = CountVectorizer(analyzer = 'word', max_features = 50000, min_df = 30, stop_words = stop_words, tokenizer = tokenize)\n",
    "    x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
    "    # we set a TfIdf Transformer, and transform the counts with the model\n",
    "    transformer = TfidfTransformer(smooth_idf = False, sublinear_tf = True, norm = 'l2')\n",
    "    x_tfidf = transformer.fit_transform(x_counts)\n",
    "    # normalize the TfIdf values to unit length for each row\n",
    "    xtfidf_norm = normalize(x_tfidf, norm = 'l1', axis = 1)\n",
    "    # obtain an NMF model\n",
    "    model = NMF(n_components = ntopics, init = 'nndsvd');\n",
    "    # fit the model\n",
    "    model.fit(xtfidf_norm)\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        feat_names = vectorizer.get_feature_names()\n",
    "        word_dict = {}\n",
    "        for i in range(ntopics):\n",
    "            # for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "            words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
    "            words = [feat_names[key] for key in words_ids]\n",
    "            word_dict['Topic #' + '{:02d}'.format(i + 1)] = words\n",
    "        print(pd.DataFrame(word_dict))\n",
    "    return model\n",
    "\n",
    "def create_eta(priors, etadict, ntopics):\n",
    "    eta = np.full(shape = (ntopics, len(etadict)), fill_value = 1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    for word, topic in priors.items(): # for each word in the list of priors\n",
    "        keyindex = [index for index,term in etadict.items() if term == word] # look up the word in the dictionary\n",
    "        if (len(keyindex) > 0): # if it's in the dictionary\n",
    "            eta[topic, keyindex[0]] = 1e7  # put a large number in there      \n",
    "    eta = np.divide(eta, eta.sum(axis = 0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.63\n",
      "Topic 0: ['obit', 'james', 'lee', 'robert', 'jr', 'sr', 'smith', 'thomas', 'brown', 'ann']\n",
      "Topic 1: ['obit', 'mary', 'wayne', 'ruth', 'elizabeth', 'john', 'ann', 'honor', 'project', 'murder']\n",
      "Topic 2: ['obit', 'ann', 'william', 'charles', 'jean', 'michael', 'david', 'williams', 'lee', 'sue']\n",
      "  Topic #01 Topic #02 Topic #03\n",
      "0      obit     james       lee\n",
      "1       ann        jr    robert\n",
      "2   william    edward        jr\n",
      "3    robert        sr        sr\n",
      "4      jean    thomas   william\n",
      "5        jr    robert     david\n",
      "6   charles     david   richard\n",
      "7      mary   william    edward\n",
      "8      john   michael    thomas\n",
      "9     david  williams     smith\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=3, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_obit[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 3)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.40\n",
      "Topic 0: ['storm', 'arkansas', 'flood', 'state', 'severe', 'littlerock', 'tornado', 'damage', 'forecaster', 'weather']\n",
      "Topic 1: ['arkansas', 'rain', 'snow', 'state', 'storm', 'flood', 'heavy', 'part', 'weather', 'expect']\n",
      "Topic 2: ['arkansas', 'storm', 'weather', 'state', 'tornado', 'severe', 'forecaster', 'flood', 'possible', 'hit']\n",
      "Topic 3: ['storm', 'state', 'tornado', 'arkansas', 'damage', 'severe', 'wind', 'strong', 'risk', 'flood']\n",
      "  Topic #01 Topic #02   Topic #03   Topic #04\n",
      "0     storm   tornado       flood    arkansas\n",
      "1     state     state       state     weather\n",
      "2       hit    damage       river        snow\n",
      "3     south       hit       levee       state\n",
      "4     power      kill         hit       river\n",
      "5      kill   weather        rain        rain\n",
      "6    damage      wind  littlerock  forecaster\n",
      "7    severe    strong         set      severe\n",
      "8      wind  possible       heavy        risk\n",
      "9      risk     bring      county    possible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_weather[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 4)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.06\n",
      "Topic 0: ['arkansas', 'time', 'crossword', 'story', 'puzzle', 'historic', 'ny', 'conceptis', 'sudoko', 'littlerock']\n",
      "Topic 1: ['crossword', 'universal', 'premier', 'tv', 'sudoku', 'trump', 'rhetoric', 'district', 'jumble', 'time']\n",
      "Topic 2: ['daily', 'crossword', 'sudoku', 'jumble', 'killer', 'bridge', 'crystal', 'commuter', 'hitori', 'pro']\n",
      "Topic 3: ['editorial', 'jumble', 'bridge', 'sudoku', 'conceptis', 'kid', 'crossword', 'ace', 'tv', 'victory']\n",
      "Topic 4: ['bridge', 'historic', 'jump', 'new', 'futoshiki', 'state', 'wordsleuth', 'set', 'motorist', 'man']\n",
      "   Topic #01  Topic #02   Topic #03  Topic #04 Topic #05\n",
      "0  editorial  crossword      sudoku     jumble    bridge\n",
      "1       page  universal   conceptis      daily       ace\n",
      "2      guide    premier      killer         tv   crystal\n",
      "3        new         tv         pro        kid  arkansas\n",
      "4       time   observer      sudoko  crossword      lane\n",
      "5        kid       time  littlerock   commuter     close\n",
      "6      story         la      puzzle      river     river\n",
      "7   arkansas      daily   crossword     sudoku  historic\n",
      "8       year     puzzle      jumble      guide       set\n",
      "9      state   commuter    historic       jump       new\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_games[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.97\n",
      "Topic 0: ['stock', 'rate', 'china', 'state', 'tariff', 'year', 'fall', 'plant', 'virus', 'face']\n",
      "Topic 1: ['china', 'trump', 'trade', 'deal', 'oil', 'new', 'talk', 'firm', 'set', 'say']\n",
      "Topic 2: ['business', 'brief', 'facebook', 'bid', 'china', 'eu', 'struggle', 'tariff', 'people', 'heat']\n",
      "Topic 3: ['stock', 'rise', 'sale', 'trump', 'trade', 'huawei', 'bankruptcy', 'record', 'home', 'china']\n",
      "   Topic #01  Topic #02 Topic #03   Topic #04\n",
      "0      brief   business     index       watch\n",
      "1   business     people  arkansas  bankruptcy\n",
      "2       open      award     stock        file\n",
      "3  apartment  northwest       end     concern\n",
      "4   official   arkansas      fall   developer\n",
      "5      trade      small     state    arkansas\n",
      "6  developer      state      gain       judge\n",
      "7       file         nw     close     walmart\n",
      "8    airport       care      drop      report\n",
      "9    million    company       add         cut\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_business[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 4)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.99\n",
      "Topic 0: ['year', 'honor', 'artist', 'new', 'school', 'state', 'county', 'work', 'conway', 'arkansas']\n",
      "Topic 1: ['new', 'director', 'conway', 'award', 'win', 'county', 'coach', 'miss', 'contestant', 'mayor']\n",
      "Topic 2: ['center', 'area', 'arkansas', 'event', 'river', 'county', 'community', 'gardener', 'plant', 'honor']\n",
      "Topic 3: ['center', 'new', 'conway', 'coach', 'benton', 'year', 'searcy', 'home', 'award', 'play']\n",
      "Topic 4: ['family', 'honor', 'farm', 'county', 'arkansas', 'spring', 'operation', 'center', 'russellville', 'hot']\n",
      "  Topic #01  Topic #02 Topic #03       Topic #04 Topic #05\n",
      "0    conway     center     cabot    russellville    searcy\n",
      "1       new     senior       new             new      year\n",
      "2       ann   director       ann           young       new\n",
      "3       man  community   teacher            year     woman\n",
      "4     woman       open  director            park  business\n",
      "5    artist        new     woman           mayor      help\n",
      "6    school      child     coach           woman      park\n",
      "7  director     bryant      year            city      open\n",
      "8      year    present     state  superintendent       man\n",
      "9       art     school       art           chief     coach\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_promotions[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.14\n",
      "Topic 0: ['opinion', 'editorial', 'nwa', 'guest', 'writer', 'nelson', 'rex', 'online', 'masterson', 'brummett']\n",
      "Topic 1: ['online', 'brummett', 'opinion', 'masterson', 'john', 'nwa', 'editorial', 'read', 'mike', 'new']\n",
      "Topic 2: ['opinion', 'letter', 'brummett', 'editorial', 'john', 'online', 'express', 'paul', 'nelson', 'rex']\n",
      "Topic 3: ['letter', 'opinion', 'editorial', 'online', 'brummett', 'nelson', 'rex', 'masterson', 'writer', 'life']\n",
      "Topic 4: ['online', 'masterson', 'rex', 'nelson', 'mike', 'read', 'opinion', 'time', 'arkansas', 'brummett']\n",
      "   Topic #01  Topic #02 Topic #03  Topic #04  Topic #05\n",
      "0     letter    opinion  brummett  editorial  masterson\n",
      "1        nwa       read    online        nwa     online\n",
      "2     online  editorial      john     writer       mike\n",
      "3    opinion     writer      read        rex       read\n",
      "4     writer      guest    writer       read     writer\n",
      "5        rex       john       rex    opinion        rex\n",
      "6       read        rex   opinion     online    opinion\n",
      "7     nelson     nelson       nwa     nelson        nwa\n",
      "8       mike       mike    nelson       mike     nelson\n",
      "9  masterson   brummett      mike  masterson     letter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_opinion[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.50\n",
      "Topic 0: ['arkansas', 'charge', 'death', 'murder', 'case', 'face', 'arrest', 'county', 'man', 'teen']\n",
      "Topic 1: ['littlerock', 'police', 'man', 'arrest', 'woman', 'arkansas', 'year', 'north', 'kill', 'shot']\n",
      "Topic 2: ['arkansas', 'man', 'apartment', 'state', 'shoot', 'accuse', 'arrest', 'chase', 'driver', 'officer']\n",
      "Topic 3: ['man', 'year', 'arkansas', 'old', 'guilty', 'girl', 'child', 'sentence', 'plead', 'porn']\n",
      "Topic 4: ['arkansas', 'man', 'police', 'arrest', 'charge', 'woman', 'northwest', 'shoot', 'sheriff', 'littlerock']\n",
      "Topic 5: ['police', 'beat', 'suspect', 'shooting', 'death', 'fatal', 'lr', 'homicide', 'man', 'investigate']\n",
      "Topic 6: ['man', 'littlerock', 'police', 'hot', 'spring', 'sexual', 'arkansas', 'shoot', 'burglary', 'report']\n",
      "         Topic #01   Topic #02    Topic #03 Topic #04   Topic #05 Topic #06  \\\n",
      "0             beat    burglary   littlerock      year      record    arrest   \n",
      "1           police      report       police  arkansas       death   suspect   \n",
      "2               lr       north        north       man        suit  shooting   \n",
      "3          officer        hold        shoot  sentence         gun     fatal   \n",
      "4      investigate        lead          man      kill    homicide  arkansas   \n",
      "5            woman  littlerock         shot      case       trial     woman   \n",
      "6            chief       store        woman    accuse  connection   slaying   \n",
      "7         arkansan     suspect     homicide       old    arkansan  homicide   \n",
      "8            state       adult     shooting     child         rob      lead   \n",
      "9  centralarkansas      target  investigate    guilty      parent     death   \n",
      "\n",
      "  Topic #07  \n",
      "0    charge  \n",
      "1      face  \n",
      "2    murder  \n",
      "3     woman  \n",
      "4     death  \n",
      "5       man  \n",
      "6      slay  \n",
      "7      drug  \n",
      "8   officer  \n",
      "9      file  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=7, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_crime[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 7)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.90\n",
      "Topic 0: ['dearabby', 'help', 'name', 'festival', 'big', 'kid', 'arkansas', 'wife', 'photo', 'bring']\n",
      "Topic 1: ['garden', 'movie', 'old', 'face', 'home', 'hint', 'helpful', 'state', 'let', 'word']\n",
      "Topic 2: ['new', 'littlerock', 'review', 'arkansas', 'restaurant', 'music', 'year', 'film', 'high', 'open']\n",
      "Topic 3: ['arkansan', 'notable', 'dearabby', 'time', 'life', 'scene', 'language', 'watch', 'woman', 'ex']\n",
      "Topic 4: ['holiday', 'horoscope', 'wedding', 'day', 'dearabby', 'mom', 'history', 'make', 'menu', 'close']\n",
      "Topic 5: ['talk', 'hunt', 'treasure', 'play', 'note', 'entertainment', 'break', 'work', 'white', 'room']\n",
      "   Topic #01 Topic #02   Topic #03    Topic #04 Topic #05 Topic #06\n",
      "0  horoscope      hint       super      wedding  calendar      mind\n",
      "1    holiday   helpful        quiz      william  religion      open\n",
      "2    opinion   opinion        word         plan   weekend    garden\n",
      "3      party       add        film  anniversary     brief      film\n",
      "4      sweet        ex     science       thomas     music    thomas\n",
      "5        new       age  literature    elizabeth   theater    museum\n",
      "6       gift   welcome       title      michael  arkansas       god\n",
      "7   dearabby   feature      letter           dr     dance       bad\n",
      "8  christmas      host     country        marie     style      meal\n",
      "9      house      best   christmas     dearabby     guide    master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=6, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_entertainment[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 6)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.53\n",
      "Topic 0: ['columnist', 'need', 'time', 'right', 'war', 'don', 'health', 'care', 'stop', 'great']\n",
      "Topic 1: ['john', 'cartoon', 'deering', 'best', 'rex', 'nelson', 'big', 'seller', 'burner', 'home']\n",
      "Topic 2: ['life', 'good', 'day', 'let', 'school', 'work', 'state', 'trump', 'holiday', 'choice']\n",
      "Topic 3: ['letter', 'arkansas', 'columnist', 'year', 'democrat', 'isn', 'road', 'money', 'point', 'tree']\n",
      "Topic 4: ['guest', 'writer', 'hanson', 'column', 'stephens', 'bret', 'biden', 'gitz', 'know', 'bradley']\n",
      "Topic 5: ['opinion', 'victor', 'american', 'davis', 'people', 'protect', 'rule', 'debate', 'matter', 'gun']\n",
      "Topic 6: ['masterson', 'mike', 'martin', 'save', 'philip', 'columnist', 'opinion', 'old', 'watch', 'world']\n",
      "  Topic #01    Topic #02 Topic #03 Topic #04 Topic #05 Topic #06  Topic #07\n",
      "0    letter    columnist      john    burner      need    nelson   arkansas\n",
      "1    editor        trump   cartoon       day    editor       rex        tom\n",
      "2      vote  impeachment   deering      time      know      time    opinion\n",
      "3   opinion         good  brummett       let       new   opinion       life\n",
      "4     right         time     trump       new     right     state        big\n",
      "5      good          win       new   problem       don    change     editor\n",
      "6       win       bernie   opinion      year   opinion      good        let\n",
      "7      save        right      time      free     biden       win       good\n",
      "8      time         save     state    change    martin       new     lesson\n",
      "9     world          day       day       big      year       big  masterson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=7, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_editorial[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 7)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.48\n",
      "Topic 0: ['qb', 'game', 'central', 'race', 'year', 'chief', 'finish', 'add', 'change', 'close']\n",
      "Topic 1: ['game', 'new', 'title', 'run', 'set', 'time', 'big', 'win', 'home', 'lead']\n",
      "Topic 2: ['arkansas', 'football', 'prep', 'basketball', 'nfl', 'college', 'past', 'littlerock', 'game', 'west']\n",
      "Topic 3: ['wire', 'victory', 'defense', 'ualr', 'lead', 'northside', 'second', 'team', 'travs', 'ready']\n",
      "Topic 4: ['state', 'sport', 'brief', 'win', 'playoff', 'live', 'season', 'recruiting', 'end', 'update']\n",
      "Topic 5: ['hog', 'win', 'lr', 'rout', 'beat', 'cowboy', 'coach', 'league', 'christian', 'leave']\n",
      "Topic 6: ['day', 'highschool', 'point', 'look', 'card', 'start', 'good', 'cup', 'offer', 'championship']\n",
      "Topic 7: ['arkansas', 'open', 'guy', 'oaklawn', 'wolf', 'recruit', 'woman', 'visit', 'red', 'cub']\n",
      "      Topic #01 Topic #02 Topic #03 Topic #04 Topic #05   Topic #06 Topic #07  \\\n",
      "0          wire     sport       hog      post   content          ua      game   \n",
      "1         green     brief        ex      loss     coach       coach     today   \n",
      "2     offensive     state       sec      bass       hop       woman     title   \n",
      "3  championship   roundup      work  favorite  champion         men  division   \n",
      "4           win     local        qb      look     share    football      week   \n",
      "5        finish   outdoor      loss     class      bowl      player       big   \n",
      "6          late       nwa  transfer   defense      wolf    baseball       win   \n",
      "7       victory  division     texas       bad  phillies  basketball  football   \n",
      "8          hurt   fishing      star   oaklawn    change         new       nfl   \n",
      "9       stadium    report    target      west     state   musselman       key   \n",
      "\n",
      "    Topic #08  \n",
      "0    arkansas  \n",
      "1        past  \n",
      "2    postcard  \n",
      "3    football  \n",
      "4       coach  \n",
      "5         win  \n",
      "6      report  \n",
      "7  highschool  \n",
      "8  basketball  \n",
      "9    outdoors  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_sports[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.40\n",
      "Topic 0: ['county', 'trump', 'korea', 'talk', 'governor', 'gop', 'win', 'state', 'race', 'raise']\n",
      "Topic 1: ['trump', 'senate', 'plan', 'close', 'say', 'divorce', 'work', 'lane', 'eu', 'wall']\n",
      "Topic 2: ['nation', 'woman', 'day', 'kong', 'hong', 'police', 'abortion', 'troop', 'new', 'kill']\n",
      "Topic 3: ['littlerock', 'arkansas', 'migrant', 'license', 'charge', 'marriage', 'washington', 'north', 'benton', 'northwest']\n",
      "Topic 4: ['protest', 'dead', 'record', 'shoot', 'gun', 'man', 'body', 'abuse', 'campaign', 'witness']\n",
      "Topic 5: ['trump', 'say', 'case', 'set', 'judge', 'iran', 'ex', 'trial', 'biden', 'probe']\n",
      "Topic 6: ['death', 'school', 'vote', 'panel', 'state', 'aid', 'rise', 'tax', 'report', 'board']\n",
      "Topic 7: ['brief', 'arkansas', 'face', 'name', 'world', 'court', 'kill', 'crash', 'hit', 'democrat']\n",
      "  Topic #01 Topic #02 Topic #03   Topic #04  Topic #05   Topic #06  \\\n",
      "0       day    record      face       brief   straight     divorce   \n",
      "1   closing     daily    charge      nation  northwest       grant   \n",
      "2  memorial    filing       new       world   arkansas    arkansas   \n",
      "3      year       set  arkansas  washington       year     roundup   \n",
      "4   veteran     state     state       state   oklahoma      decree   \n",
      "5  arkansas  crawford        lr     capitol    roundup   northwest   \n",
      "6  election   revenue       man      county     arrest        file   \n",
      "7        nd    county      race   sebastian     gather       court   \n",
      "8     labor  arkansas   lawsuit        file      night  washington   \n",
      "9     state    arrest     count         law  protester      county   \n",
      "\n",
      "     Topic #07    Topic #08  \n",
      "0     notebook        trump  \n",
      "1   littlerock     arkansas  \n",
      "2    education        court  \n",
      "3       county         plan  \n",
      "4      pulaski        state  \n",
      "5           ua  impeachment  \n",
      "6        north        house  \n",
      "7         high        tweet  \n",
      "8  environment         talk  \n",
      "9     arkansas        judge  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_news[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -10.02\n",
      "Topic 0: ['littlerock', 'bluff', 'north', 'pine', 'city', 'expensive', 'mr', 'sell', 'head', 'beebe']\n",
      "Topic 1: ['arkansas', 'new', 'state', 'place', 'case', 'day', 'smith', 'chief', 'death', 'free']\n",
      "Topic 2: ['week', 'state', 'say', 'time', 'close', 'arkansas', 'court', 'littlerock', 'weather', 'virus']\n",
      "Topic 3: ['open', 'school', 'year', 'photo', 'restaurant', 'online', 'spring', 'set', 'video', 'work']\n",
      "Topic 4: ['benton', 'value', 'leave', 'statement', 'core', 'look', 'group', 'line', 'ann', 'library']\n",
      "Topic 5: ['best', 'bet', 'site', 'new', 'high', 'west', 'good', 'long', 'flight', 'tell']\n",
      "Topic 6: ['home', 'woman', 'man', 'littlerock', 'cost', 'view', 'cover', 'old', 'center', 'officer']\n",
      "Topic 7: ['food', 'change', 'report', 'hall', 'life', 'board', 'drive', 'town', 'james', 'add']\n",
      "    Topic #01   Topic #02  Topic #03 Topic #04 Topic #05   Topic #06  \\\n",
      "0    arkansas     weather     benton     state      best         new   \n",
      "1        week      travel        ann      case       bet        home   \n",
      "2        vote        draw      james  governor      know        open   \n",
      "3         day       watch     robert      test    school        year   \n",
      "4   northwest        road    charles     order    player         job   \n",
      "5       court       flood       faye     index      plan        case   \n",
      "6      appeal      spring        lee      year     right         set   \n",
      "7       death        step  elizabeth       hit    season  littlerock   \n",
      "8        case         add     martin      home     award        rule   \n",
      "9  littlerock  california         jo    school     visit       coach   \n",
      "\n",
      "  Topic #07   Topic #08  \n",
      "0        la      online  \n",
      "1        le       video  \n",
      "2        en        home  \n",
      "3       lee       close  \n",
      "4      time       class  \n",
      "5        el          le  \n",
      "6        di        hold  \n",
      "7        se          ua  \n",
      "8   service  littlerock  \n",
      "9      feed     service  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_others[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.76\n",
      "Topic 0: ['virus', 'plan', 'new', 'trump', 'order', 'pandemic', 'vote', 'tell', 'rule', 'school']\n",
      "Topic 1: ['stock', 'street', 'wall', 'coronavirus', 'end', 'relief', 'house', 'market', 'loan', 'pandemic']\n",
      "Topic 2: ['state', 'virus', 'coronavirus', 'case', 'death', 'offer', 'site', 'rise', 'official', 'health']\n",
      "Topic 3: ['virus', 'reopen', 'state', 'governor', 'push', 'arkansas', 'sale', 'rate', 'high', 'online']\n",
      "Topic 4: ['coronavirus', 'virus', 'arkansas', 'trump', 'amazon', 'ship', 'firm', 'concern', 'food', 'test']\n",
      "Topic 5: ['virus', 'world', 'home', 'trump', 'million', 'aid', 'littlerock', 'worker', 'jobless', 'claim']\n",
      "Topic 6: ['brief', 'virus', 'face', 'nation', 'name', 'test', 'job', 'price', 'day', 'close']\n",
      "Topic 7: ['aid', 'cut', 'business', 'gain', 'state', 'test', 'small', 'stock', 'rally', 'virus']\n",
      "  Topic #01    Topic #02 Topic #03    Topic #04 Topic #05  Topic #06  \\\n",
      "0     virus  coronavirus      face        brief  pandemic      state   \n",
      "1     trump         test      mask       nation      mask       case   \n",
      "2      test        offer       new        world      amid      offer   \n",
      "3      case         site      time   washington       day       test   \n",
      "4    spread     positive      urge  restriction     stock       site   \n",
      "5       hit    screening      test          new     force  screening   \n",
      "6     china       update      help       county     drive      death   \n",
      "7       aid           la    worker        sport      hold       rise   \n",
      "8     death           dy      ship          job     trump     united   \n",
      "9      amid        china  lockdown         hard       hit        aid   \n",
      "\n",
      "  Topic #07  Topic #08  \n",
      "0    reopen   arkansas  \n",
      "1     trump       case  \n",
      "2       set   governor  \n",
      "3    casino  northwest  \n",
      "4      plan       plan  \n",
      "5      park       test  \n",
      "6   economy      death  \n",
      "7  business       open  \n",
      "8      test   lockdown  \n",
      "9       cdc       rise  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_covid[\"headline\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_opposite = {\n",
    "    'crash':0, 'vehicle':0, 'car':0, 'truck':0, 'unlicensed':0, 'pile':0, 'hit':0, 'driver':0,\n",
    "    'drugs':3, 'heroin':3, 'fentanyl':3, 'street':3,\n",
    "    'shooting':4, 'gun':4, 'shooter':4, 'shot':4, 'accidental':4, 'armed':4, 'gunpoint':4, 'shoot':4, 'homicide':4,\n",
    "    'stabbed':5, 'stealing':5, 'murder':5, 'theft':5, 'victim':5, 'suspect':5, 'fatal':5, 'steal':5, 'stab':5,\n",
    "    'fire':2, 'blaze':2, 'firefighter':2, 'house':2,\n",
    "    'rape':1, 'child':1, 'abuse':1, 'pornography':1, 'teen':1, 'woman':1,\n",
    "}\n",
    "eta = create_eta(apriori_opposite, dictionary, 6)\n",
    "test_eta_lda(eta, dictionary, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

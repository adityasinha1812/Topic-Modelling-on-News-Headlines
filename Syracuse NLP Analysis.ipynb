{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse = pd.read_csv(\"C:/Users/adity/Downloads/SRC Project/LDA/pub1.csv\", engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean headline column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse[\"Headlines\"] = content_syracuse[\"Headlines\"].astype('str')\n",
    "content_syracuse[\"Headlines\"] = content_syracuse[\"Headlines\"].str.encode(\"ascii\", \"ignore\").str.decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new column for level 1 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse[\"level_1\"] = content_syracuse['pagePathLevel1']\n",
    "# remove / from page path\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('/','')\n",
    "# make blank rows as others\n",
    "content_syracuse['level_1'].fillna('others', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating bigrams / trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('new york','ny')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('syracuse university','su')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('president trump','trump')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('donald trump','trump')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('new year','new year')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"trump's\",'trump')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('president obama','obama')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('dear abby','dearabby')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('pm buzz','pmbuzz')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('viral video','viralvideo')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('star wars','starwars')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('white house','whitehouse')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('high school','highschool')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"women's\",'woman')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"men's\",'men')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"buffalo-bill\",'buffalobill')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"boeheims\",'boeheim')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"boeheim's\",'boeheim')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"jim boeheim\",'boeheim')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"information\",'info')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace(\"notre dame\",'notredame')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('syracuse','')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('cny','')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('ny','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up level 1 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('orangebasketball','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('orangefootball','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('golf','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('orangelacrosse','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('ny-sports','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('bowling','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('buffalo-bills','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('olympics','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('superbowl','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('sports-talk','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('su-sports-talk','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('rec-sports','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('collegefootball','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('highschoolsports','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('orangesports','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('hssports-special','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('collegelacrosse','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('collegebasketball','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('crunch','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('orangewomen','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('marchmadness','sports')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('college-sports','sports')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('us-news','news')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('business-news','business')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('celebrity-news','entertainment')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('poliquin','opinion')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('axeman','opinion')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('kirst','opinion')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('borkowski','opinion')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('food','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('drinks','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('life-and-culturefitness','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('health','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('life_and_culture','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('restaurants','life-and-culture')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('living','life-and-culture')\n",
    "\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('statefair','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('state','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('expo','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('schools','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('cny','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('neighbors','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('outdoors','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('realelocalnews-news','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('zoo','localnews')\n",
    "content_syracuse['level_1'] = content_syracuse['level_1'].str.replace('localnews-speaks','localnews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating level 1 tag for obits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(content_syracuse)):\n",
    "    if (re.search(\"today's obit:\", str(content_syracuse.Headlines[i]), re.IGNORECASE)):\n",
    "        content_syracuse[\"level_1\"][i] = \"obituaries\"\n",
    "    elif (re.search(\"today's obits:\", str(content_syracuse.Headlines[i]), re.IGNORECASE)):\n",
    "        content_syracuse[\"level_1\"][i] = \"obituaries\"\n",
    "    elif (re.search(\"today's obituaries:\", str(content_syracuse.Headlines[i]), re.IGNORECASE)):\n",
    "        content_syracuse[\"level_1\"][i] = \"obituaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create count of headlines per level 1 tag and filter < 1000 as others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse['counts'] = content_syracuse['level_1'].map(content_syracuse['level_1'].value_counts())\n",
    "content_syracuse.loc[content_syracuse.counts < 1000, 'level_1'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ML to classify \"others\" as level 1 tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', \"%\", 'm', 'p', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    " 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"about\", \"across\", \"after\", \"all\", \n",
    " \"also\", \"an\", \"and\", \"another\", \"added\", \"any\", \"are\", \"as\", \"at\", \"basically\", \"be\", \"because\", 'become', \"been\", \"before\", \n",
    " \"being\", \"between\", \"both\", \"but\", \"by\", \"came\", \"can\", \"come\", \"could\", \"did\", \"do\", \"does\", \"each\", \"else\", \"every\",\n",
    " \"either\", \"especially\", \"for\", \"from\", \"get\", \"given\", \"gets\", 'give', 'gives', \"got\", \"goes\", \"had\", \"has\", \"have\", \"he\", \n",
    " \"her\", \"here\", \"him\", \"himself\", \"his\", \"how\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"lands\", \"like\", \"make\", \n",
    " \"making\", \"made\", \"many\", \"may\", \"me\", \"might\", \"more\", \"most\", \"much\", \"must\", \"my\", \"never\", \"provide\", \"provides\", \n",
    " \"perhaps\", \"no\", \"now\", \"of\", \"on\", \"only\", \"or\", \"other\", \"our\", \"out\", \"over\", \"re\", \"said\", \"same\", \"see\", \"should\", \n",
    " \"since\", \"so\", \"some\", \"still\", \"such\", \"seeing\", \"see\", \"take\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \n",
    " \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"up\", \"use\", \"using\", \"used\", \"underway\", \"very\", \"want\", \n",
    " \"was\", \"way\", \"we\", \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"whilst\", \"who\", \"will\", \"with\", \"would\", \n",
    " \"you\", \"your\", 'etc', 'via', 'eg', 'news', \"'s\", \"april\", \"march\", \"june\", \"july\", \"august\", \"february\", \"january\", \"december\", \n",
    " \"september\", \"october\", \"dec\", \"oct\", \"nov\", \"today's\", \"'the'\", \"sept\", \"feb\", \"jan\", \"amp\", \"say\", \"'the\", \"friday's\", \"friday\",\n",
    " \"saturday\", \"saturday's\", \"sunday\", \"sunday's\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(extra_stop_words)\n",
    "\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('obituaries','obits')\n",
    "content_syracuse['Headlines'] = content_syracuse['Headlines'].str.replace('obituary','obits')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [wn.lemmatize(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating subset without \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news                81587\n",
       "others              33531\n",
       "sports              33307\n",
       "entertainment       23353\n",
       "localnews           21659\n",
       "crime               15204\n",
       "opinion             14184\n",
       "life-and-culture    11237\n",
       "business             7108\n",
       "politics             4881\n",
       "weather              2363\n",
       "obituaries           1150\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_subset_without_others = content_syracuse[content_syracuse[\"level_1\"] != \"others\"]\n",
    "content_syracuse[\"pred\"] = content_syracuse[\"level_1\"]\n",
    "content_syracuse[\"conf\"] = 0\n",
    "content_syracuse[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news                81587\n",
       "sports              33307\n",
       "entertainment       23353\n",
       "localnews           21659\n",
       "crime               15204\n",
       "opinion             14184\n",
       "life-and-culture    11237\n",
       "business             7108\n",
       "politics             4881\n",
       "weather              2363\n",
       "obituaries           1150\n",
       "Name: level_1, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_subset_without_others[\"level_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_subset_without_others['Headlines'], content_subset_without_others['level_1'], random_state = 0)\n",
    "count_vect = CountVectorizer(min_df = 30, stop_words = stop_words, tokenizer = tokenize, analyzer = 'word', max_features = 50000)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf = TfidfTransformer(sublinear_tf = True, norm = 'l2', smooth_idf = False)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf = CalibratedClassifierCV(LinearSVC()).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news                69243\n",
       "sports              45382\n",
       "crime               33545\n",
       "entertainment       29783\n",
       "localnews           22534\n",
       "life-and-culture    13350\n",
       "business            11582\n",
       "opinion             10500\n",
       "politics             7798\n",
       "weather              4656\n",
       "obituaries           1191\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_syracuse[\"pred\"] = clf.predict(count_vect.transform(content_syracuse[\"Headlines\"]))\n",
    "content_syracuse[\"conf\"] = (clf.predict_proba(count_vect.transform(content_syracuse[\"Headlines\"]))).max(axis = 1)\n",
    "content_syracuse[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_syracuse[\"final_pred\"] = np.where((content_syracuse[\"level_1\"] == \"others\") & (content_syracuse[\"conf\"] >= 0.6), content_syracuse[\"pred\"], content_syracuse[\"level_1\"])\n",
    "content_syracuse['counts'] = content_syracuse['final_pred'].map(content_syracuse['final_pred'].value_counts())\n",
    "content_syracuse.to_csv(\"content_syracuse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news                85283\n",
       "sports              36815\n",
       "entertainment       26192\n",
       "localnews           22921\n",
       "others              20202\n",
       "crime               15595\n",
       "opinion             14659\n",
       "life-and-culture    11764\n",
       "business             7455\n",
       "politics             5086\n",
       "weather              2441\n",
       "obituaries           1151\n",
       "Name: final_pred, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_syracuse[\"final_pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting each of the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_subset_obit = content_syracuse[content_syracuse[\"final_pred\"] == \"obituaries\"]\n",
    "content_subset_weather = content_syracuse[content_syracuse[\"final_pred\"] == \"weather\"]\n",
    "content_subset_politics = content_syracuse[content_syracuse[\"final_pred\"] == \"politics\"]\n",
    "content_subset_business = content_syracuse[content_syracuse[\"final_pred\"] == \"business\"]\n",
    "content_subset_landc = content_syracuse[content_syracuse[\"final_pred\"] == \"life-and-culture\"]\n",
    "content_subset_opinion = content_syracuse[content_syracuse[\"final_pred\"] == \"opinion\"]\n",
    "content_subset_crime = content_syracuse[content_syracuse[\"final_pred\"] == \"crime\"]\n",
    "content_subset_localnews = content_syracuse[content_syracuse[\"final_pred\"] == \"localnews\"]\n",
    "content_subset_entertainment = content_syracuse[content_syracuse[\"final_pred\"] == \"entertainment\"]\n",
    "content_subset_sports = content_syracuse[content_syracuse[\"final_pred\"] == \"sports\"]\n",
    "content_subset_news = content_syracuse[content_syracuse[\"final_pred\"] == \"news\"]\n",
    "content_subset_others = content_syracuse[content_syracuse[\"final_pred\"] == \"others\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to try\n",
    "1. K Means\n",
    "2. Guided LDA\n",
    "3. LDA\n",
    "4. NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : wa, obit, teacher, year, school, highschool, john, james, north\n",
      "Cluster 1 : worked, obit, year, school, james, jr, graduated, michael, highschool\n",
      "Cluster 2 : obit, graduated, highschool, year, owned, school, retired, served, jr\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_obit[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 3\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : upstate, storm, rain, week, heat, winter, today, severe, wind\n",
      "Cluster 1 : snow, effect, lake, storm, upstate, central, inch, winter, foot\n",
      "Cluster 2 : weather, central, weekend, week, upstate, winter, cold, day, start\n",
      "Cluster 3 : central, storm, day, school, winter, record, county, wind, year\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_weather[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : race, senate, cuomo, democratic, state, congress, schumer, debate, democrat\n",
      "Cluster 1 : john, katko, rep, balter, dana, congress, house, video, trump\n",
      "Cluster 2 : trump, border, whitehouse, video, campaign, president, democrat, wall, report\n",
      "Cluster 3 : election, county, onondaga, day, ballot, voter, win, primary, result\n",
      "Cluster 4 : gop, trump, tax, plan, candidate, rep, primary, senate, house\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_politics[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : new, compa, named, store, open, close, job, dewitt, building\n",
      "Cluster 1 : joined, compa, group, associate, terakeet, st, pinckney, hugo, barton\n",
      "Cluster 2 : onondaga, county, judgment, department, inspection, bankruptcy, health, certificate, restaurant\n",
      "Cluster 3 : promoted, compa, ha, bank, group, cxtec, loguidice, barton, dannible\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_business[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life and Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : inspiration, daily, god, love, community, life, change, good, new\n",
      "Cluster 1 : new, upstate, open, beer, dearabby, video, buzz, restaurant, dinner\n",
      "Cluster 2 : review, dining, food, fair, offer, restaurant, wine, good, italian\n",
      "Cluster 3 : best, central, restaurant, pizza, upstate, hunt, chicken, burger, winner\n",
      "Cluster 4 : photo, prom, day, video, highschool, essay, senior, annual, new\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_landc[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : editorial, year, new, day, poliquin, ha, state, time, su\n",
      "Cluster 1 : letter, school, onondaga, county, thank, wa, trump, state, day\n",
      "Cluster 2 : football, basketball, recap, su, brent, axe, orange, coach, team\n",
      "Cluster 3 : commentary, trump, need, health, tax, reform, day, people, onondaga\n",
      "Cluster 4 : cartoon, editorial, trump, mueller, shooting, wall, border, facebook, climate\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_opinion[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : county, oswego, onondaga, man, deputy, oneida, cayuga, charged, madison\n",
      "Cluster 1 : judge, murder, court, lawyer, case, trial, plea, accused, prison\n",
      "Cluster 2 : man, charged, woman, shooting, shot, firefighter, cop, home, house\n",
      "Cluster 3 : crash, killed, car, county, driver, fatal, police, man, injured\n",
      "Cluster 4 : year, old, man, police, girl, boy, killed, prison, woman\n",
      "Cluster 5 : police, man, investigate, shooting, shot, woman, officer, arrest, state\n",
      "Cluster 6 : murder, accused, trial, man, neulander, dr, suspect, da, case\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_crime[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 7\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : best, bet, saturday, saturday's, friday, sunday's, sunday, concert, friday's\n",
      "Cluster 1 : star, video, music, day, buzz, festival, dy, photo, dead\n",
      "Cluster 2 : new, movie, buzz, tv, music, star, song, video, trailer\n",
      "Cluster 3 : concert, tour, date, upstate, lakeview, amphitheater, include, buffalo, calendar\n",
      "Cluster 4 : pmbuzz, trailer, buzz, movie, new, tv, star, reboot, arrested\n",
      "Cluster 5 : review, playhouse, theatre, stage, cortland, rarely, chevy, fun, fair\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_entertainment[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 6\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : lake, new, state, photo, basketball, fishing, man, year, buzz\n",
      "Cluster 1 : prom, photo, junior, highschool, senior, ball, central, academy, dance\n",
      "Cluster 2 : upstate, police, man, ranked, best, woman, school, photo, year\n",
      "Cluster 3 : fair, food, review, state, row, chevy, court, day, building\n",
      "Cluster 4 : house, week, home, colonial, road, st, cazenovia, manlius, ha\n",
      "Cluster 5 : graduation, highschool, class, list, valedictorian, salutatorian, central, candidate, sr\n",
      "Cluster 6 : school, student, highschool, musical, present, spring, perform, district, fall\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_localnews[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 7\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : crunch, buffalo, team, nfl, new, coach, video, nba, tournament\n",
      "Cluster 1 : basketball, woman, tournament, team, ncaa, game, orange, acc, college\n",
      "Cluster 2 : player, basketball, football, su, lacrosse, nba, woman, team, year\n",
      "Cluster 3 : win, game, crunch, title, championship, basketball, state, speedway, video\n",
      "Cluster 4 : football, recruiting, su, team, coach, star, qb, game, dino\n",
      "Cluster 5 : lacrosse, men, woman, team, su, boy, girl, win, ncaa\n",
      "Cluster 6 : jim, boeheim, coach, basketball, boeheim's, conference, boeheims, watch, win\n",
      "Cluster 7 : channel, tv, stream, live, time, info, information, game, tipoff\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_sports[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : crash, killed, car, fatal, man, injured, driver, woman, update\n",
      "Cluster 1 : state, fair, class, championship, girl, boy, win, semifinal, ranking\n",
      "Cluster 2 : police, man, officer, shooting, woman, arrest, charge, identify, chief\n",
      "Cluster 3 : man, charged, accused, year, charge, prison, sentenced, county, oswego\n",
      "Cluster 4 : win, lottery, number, million, jackpot, winning, powerball, mega, girl\n",
      "Cluster 5 : county, onondaga, oswego, lake, cayuga, madison, sheriff, deputy, judgment\n",
      "Cluster 6 : new, school, year, today, compa, central, highschool, su, student\n",
      "Cluster 7 : section, iii, boy, girl, class, basketball, star, state, championship\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_news[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : photo, central, week, chief, video, upstate, state, best, time\n",
      "Cluster 1 : su, student, video, sport, great, time, alum, campus, frat\n",
      "Cluster 2 : new, desti, usa, store, open, mall, tv, photo, year\n",
      "Cluster 3 : year, county, onondaga, old, madison, oswego, lake, state, cayuga\n",
      "Cluster 4 : ski, report, th, rd, nd, saturday, friday, claim, ticket\n",
      "Cluster 5 : war, memorial, live, john, james, brown, onondaga, animal, lady\n",
      "Cluster 6 : day, home, parade, viralvideo, custom, builder, photo, summercuse, house\n",
      "Cluster 7 : man, crash, police, traffic, alert, interstate, update, charged, lane\n"
     ]
    }
   ],
   "source": [
    "content_subset_headline = content_subset_others[['Headlines']]\n",
    "document = list(content_subset_headline[\"Headlines\"])\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, min_df = 30, norm = 'l2', stop_words = stop_words, tokenizer = tokenize)\n",
    "X = vectorizer.fit_transform(document)\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 500, n_init = 20)\n",
    "model.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "words = model.cluster_centers_.argsort()[:,-1:-10:-1]\n",
    "for num, centroid in enumerate(words):\n",
    "    print('Cluster ' + str(num) + ' : ' + ', '.join(terms[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify Penn tags to n (NOUN), v (VERB), a (ADJECTIVE) or r (ADVERB)\n",
    "def simplify(penn_tag):\n",
    "    pre = penn_tag[0]\n",
    "    if (pre == 'J'):\n",
    "        return 'a'\n",
    "    elif (pre == 'R'):\n",
    "        return 'r'\n",
    "    elif (pre == 'V'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def preprocess(text):\n",
    "    toks = gensim.utils.simple_preprocess(str(text), deacc = True)\n",
    "    wn = WordNetLemmatizer()\n",
    "    return [wn.lemmatize(tok, simplify(pos)) for tok, pos in nltk.pos_tag(toks) if tok not in stop_words]\n",
    "\n",
    "def test_eta_lda(eta, dictionary, ntopics, print_topics = True, print_dist = True):\n",
    "    np.random.seed(42) # set the random seed for repeatability\n",
    "    bow = [dictionary.doc2bow(line) for line in corp] # get the bow-format lines with the set dictionary\n",
    "    with (np.errstate(divide = 'ignore')):  # ignore divide-by-zero warnings\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus = bow, id2word = dictionary, num_topics = ntopics,\n",
    "            random_state = 42, eta = eta, alpha = 'auto')\n",
    "    # visuzlize the model term topics\n",
    "    print('Perplexity: {:.2f}'.format(model.log_perplexity(bow)))\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        for topic in range(ntopics):\n",
    "            print('Topic {}: {}'.format(topic, [dictionary[w] for w, p in model.get_topic_terms(topic, topn = 10)]))\n",
    "#     if print_dist:\n",
    "#         # display the topic probabilities for each document\n",
    "#         for line, bag in zip(txt, bow):\n",
    "#             doc_topics = ['({}, {:.1%})'.format(topic, prob) for topic, prob in model.get_document_topics(bag)]\n",
    "#             print('{} {}'.format(line, doc_topics))\n",
    "    return model\n",
    "\n",
    "def test_eta_nmf(eta, dictionary, train_headlines_sentences, ntopics, print_topics = True, print_dist = True):\n",
    "    vectorizer = CountVectorizer(analyzer = 'word', max_features = 50000, min_df = 30, stop_words = stop_words, tokenizer = tokenize)\n",
    "    x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
    "    # we set a TfIdf Transformer, and transform the counts with the model\n",
    "    transformer = TfidfTransformer(smooth_idf = False, sublinear_tf = True, norm = 'l2')\n",
    "    x_tfidf = transformer.fit_transform(x_counts)\n",
    "    # normalize the TfIdf values to unit length for each row\n",
    "    xtfidf_norm = normalize(x_tfidf, norm = 'l1', axis = 1)\n",
    "    # obtain an NMF model\n",
    "    model = NMF(n_components = ntopics, init = 'nndsvd');\n",
    "    # fit the model\n",
    "    model.fit(xtfidf_norm)\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        feat_names = vectorizer.get_feature_names()\n",
    "        word_dict = {}\n",
    "        for i in range(ntopics):\n",
    "            # for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "            words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
    "            words = [feat_names[key] for key in words_ids]\n",
    "            word_dict['Topic #' + '{:02d}'.format(i + 1)] = words\n",
    "        print(pd.DataFrame(word_dict))\n",
    "    return model\n",
    "\n",
    "def create_eta(priors, etadict, ntopics):\n",
    "    eta = np.full(shape = (ntopics, len(etadict)), fill_value = 1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    for word, topic in priors.items(): # for each word in the list of priors\n",
    "        keyindex = [index for index,term in etadict.items() if term == word] # look up the word in the dictionary\n",
    "        if (len(keyindex) > 0): # if it's in the dictionary\n",
    "            eta[topic, keyindex[0]] = 1e7  # put a large number in there      \n",
    "    eta = np.divide(eta, eta.sum(axis = 0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.99\n",
      "Topic 0: ['today', 'obits', 'year', 'work', 'james', 'obit', 'school', 'dr', 'own', 'teacher']\n",
      "Topic 1: ['today', 'obits', 'work', 'school', 'year', 'obit', 'teacher', 'north', 'graduate', 'retire']\n",
      "Topic 2: ['today', 'obits', 'graduate', 'highschool', 'year', 'work', 'jr', 'own', 'john', 'school']\n",
      "    Topic #01   Topic #02 Topic #03\n",
      "0        obit        work      year\n",
      "1       today       today    retire\n",
      "2    graduate        obit    school\n",
      "3  highschool      school   teacher\n",
      "4      school     michael     today\n",
      "5          jr          jr      obit\n",
      "6        john       james     north\n",
      "7       serve   liverpool     james\n",
      "8     teacher    graduate    county\n",
      "9          dr  highschool   michael\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=3, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_obit[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 3)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.61\n",
      "Topic 0: ['upstate', 'winter', 'storm', 'snow', 'weather', 'wet', 'forecast', 'warm', 'year', 'week']\n",
      "Topic 1: ['upstate', 'snow', 'central', 'winter', 'storm', 'video', 'wind', 'county', 'week', 'warm']\n",
      "Topic 2: ['upstate', 'snow', 'weather', 'storm', 'lake', 'central', 'effect', 'rain', 'winter', 'heat']\n",
      "Topic 3: ['weather', 'central', 'weekend', 'upstate', 'wind', 'winter', 'snow', 'cold', 'day', 'school']\n",
      "  Topic #01 Topic #02 Topic #03 Topic #04\n",
      "0   weather   upstate      snow   central\n",
      "1      week      week     storm   weekend\n",
      "2   weekend     storm      lake       day\n",
      "3    winter    winter    effect      cold\n",
      "4    spring      rain    winter    winter\n",
      "5      cool     flood  blizzard    record\n",
      "6     start     today      inch      warm\n",
      "7      cold      cold      rain     power\n",
      "8     close      warm      foot    school\n",
      "9     photo      wind     video     snowy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_weather[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 4)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.77\n",
      "Topic 0: ['trump', 'john', 'katko', 'clinton', 'rep', 'congress', 'say', 'hillary', 'new', 'vote']\n",
      "Topic 1: ['trump', 'county', 'election', 'primary', 'onondaga', 'race', 'win', 'upstate', 'gop', 'candidate']\n",
      "Topic 2: ['trump', 'border', 'wall', 'say', 'miner', 'schumer', 'stephanie', 'whitehouse', 'race', 'su']\n",
      "Topic 3: ['trump', 'say', 'debate', 'republican', 'democratic', 'rally', 'presidential', 'tariff', 'tax', 'woman']\n",
      "Topic 4: ['trump', 'say', 'rep', 'whitehouse', 'gop', 'john', 'tenney', 'video', 'campaign', 'cut']\n",
      "     Topic #01   Topic #02 Topic #03     Topic #04 Topic #05\n",
      "0        trump        john  election        senate     state\n",
      "1     campaign       katko    county      democrat     cuomo\n",
      "2   whitehouse         rep       win     president       tax\n",
      "3        rally    congress  onondaga       schumer     union\n",
      "4      america    campaign       day           gop  lawmaker\n",
      "5       report       video     voter           run       gov\n",
      "6      mueller      balter   primary          race      plan\n",
      "7        photo       house    ballot    republican       ban\n",
      "8       russia        dana    result        debate    budget\n",
      "9  immigration  whitehouse        th  presidential   upstate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_politics[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.68\n",
      "Topic 0: ['county', 'onondaga', 'store', 'close', 'restaurant', 'department', 'bankruptcy', 'new', 'compa', 'open']\n",
      "Topic 1: ['compa', 'join', 'new', 'hotel', 'inside', 'look', 'central', 'million', 'dewitt', 'photo']\n",
      "Topic 2: ['county', 'onondaga', 'inspection', 'restaurant', 'tax', 'mall', 'new', 'compa', 'health', 'business']\n",
      "Topic 3: ['compa', 'join', 'health', 'department', 'name', 'group', 'promote', 'post', 'ford', 'standard']\n",
      "     Topic #01    Topic #02  Topic #03   Topic #04\n",
      "0         join       county    promote         new\n",
      "1        compa     onondaga      compa        open\n",
      "2    associate   bankruptcy       bank       store\n",
      "3        group     judgment      cxtec       close\n",
      "4     terakeet     business      group       hotel\n",
      "5  engineering  certificate      award  restaurant\n",
      "6      inficon      warrant  loguidice       photo\n",
      "7           st          tax     barton      dewitt\n",
      "8         bank   restaurant    manager       desti\n",
      "9      michael   inspection   director         usa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_business[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 4)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life and Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.32\n",
      "Topic 0: ['photo', 'video', 'day', 'upstate', 'new', 'love', 'inside', 'kid', 'dinner', 'pick']\n",
      "Topic 1: ['open', 'new', 'restaurant', 'upstate', 'home', 'photo', 'close', 'county', 'bar', 'buzz']\n",
      "Topic 2: ['review', 'wine', 'beer', 'din', 'food', 'week', 'history', 'taste', 'new', 'video']\n",
      "Topic 3: ['best', 'central', 'inspiration', 'restaurant', 'hospital', 'upstate', 'medical', 'daily', 'video', 'chicken']\n",
      "Topic 4: ['photo', 'upstate', 'new', 'week', 'day', 'home', 'chef', 'lake', 'open', 'center']\n",
      "     Topic #01     Topic #02   Topic #03   Topic #04   Topic #05\n",
      "0  inspiration        dinner       photo        food        buzz\n",
      "1        daily    restaurant       video      review         new\n",
      "2         love          love         day        fair     trailer\n",
      "3          god       kitchen        prom         din       movie\n",
      "4    community  fayetteville     upstate  restaurant     central\n",
      "5         life         house     central         new        star\n",
      "6        power           pub        lake        best         die\n",
      "7       change         taste        city        beer       video\n",
      "8         time      camillus    festival        wine          su\n",
      "9         good   skaneateles  highschool       state  restaurant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_landc[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.57\n",
      "Topic 0: ['letter', 'editorial', 'cartoon', 'commentary', 'year', 'im', 'change', 'new', 'community', 'climate']\n",
      "Topic 1: ['basketball', 'football', 'orange', 'su', 'recap', 'coach', 'jim', 'game', 'boeheim', 'team']\n",
      "Topic 2: ['letter', 'commentary', 'trump', 'world', 'reader', 'school', 'war', 'stop', 'city', 'need']\n",
      "Topic 3: ['letter', 'commentary', 'day', 'su', 'voting', 'today', 'thank', 'year', 'world', 'veteran']\n",
      "Topic 4: ['letter', 'commentary', 'onondaga', 'county', 'editorial', 'school', 'vote', 'state', 'football', 'year']\n",
      "   Topic #01    Topic #02   Topic #03   Topic #04 Topic #05\n",
      "0     letter    editorial  commentary    football    strong\n",
      "1     school      cartoon       trump          su    memory\n",
      "2      trump        trump        need       recap     woman\n",
      "3   onondaga   basketball         tax  basketball      good\n",
      "4        day      mueller        stop      orange     right\n",
      "5      today  endorsement         day        year      lead\n",
      "6  community     facebook      health         new   justice\n",
      "7     county     shooting        city       state       law\n",
      "8      state     election     america        game      time\n",
      "9      thank         wall         gop    poliquin       gun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_opinion[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 5)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.33\n",
      "Topic 0: ['county', 'crash', 'charge', 'man', 'onondaga', 'deputy', 'sheriff', 'kill', 'oneida', 'oswego']\n",
      "Topic 1: ['murder', 'trial', 'case', 'year', 'judge', 'man', 'day', 'court', 'death', 'heidi']\n",
      "Topic 2: ['man', 'charge', 'police', 'murder', 'accuse', 'woman', 'trooper', 'stab', 'girlfriend', 'death']\n",
      "Topic 3: ['police', 'year', 'man', 'old', 'woman', 'crash', 'kill', 'stab', 'teen', 'car']\n",
      "Topic 4: ['police', 'man', 'arrest', 'charge', 'woman', 'suspect', 'gun', 'steal', 'investigate', 'video']\n",
      "Topic 5: ['update', 'crash', 'county', 'student', 'firefighter', 'woman', 'police', 'close', 'charge', 'kill']\n",
      "Topic 6: ['police', 'man', 'shoot', 'shot', 'witness', 'murder', 'south', 'street', 'hit', 'kill']\n",
      "     Topic #01 Topic #02 Topic #03 Topic #04 Topic #05  Topic #06    Topic #07\n",
      "0       police    county       man      year    arrest     murder        shoot\n",
      "1  investigate     crash    charge       old  homicide      judge  investigate\n",
      "2        woman    oswego      shot      girl      drug      trial       street\n",
      "3        state      kill    accuse       boy      bust         da        south\n",
      "4         stab  onondaga      stab    prison   blotter       case       victim\n",
      "5      officer    deputy     child      kill       com    suspect     homicide\n",
      "6          car       car     death     woman       gun     lawyer       report\n",
      "7        chief    oneida       cop      miss       cop  neulander        north\n",
      "8         miss    cayuga       gun       rap    police      court  firefighter\n",
      "9       search   sheriff     steal     death        su     accuse         teen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=7, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_crime[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 7)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.78\n",
      "Topic 0: ['new', 'book', 'movie', 'tv', 'theater', 'author', 'band', 'su', 'turn', 'series']\n",
      "Topic 1: ['buzz', 'trailer', 'voice', 'pmbuzz', 'star', 'day', 'movie', 'new', 'su', 'fair']\n",
      "Topic 2: ['dead', 'star', 'walk', 'tv', 'buzz', 'actor', 'pmbuzz', 'new', 'death', 'dy']\n",
      "Topic 3: ['musical', 'thing', 'city', 'singer', 'weekend', 'actor', 'finale', 'dead', 'new', 'band']\n",
      "Topic 4: ['star', 'pmbuzz', 'round', 'video', 'trailer', 'battle', 'merry', 'upstate', 'review', 'singer']\n",
      "Topic 5: ['review', 'festival', 'season', 'music', 'best', 'theatre', 'art', 'film', 'concert', 'stage']\n",
      "     Topic #01  Topic #02 Topic #03 Topic #04 Topic #05 Topic #06\n",
      "0          din     review  festival     video      star     music\n",
      "1   restaurant      stage   concert     photo       new     award\n",
      "2    liverpool  playhouse      best      game     movie      week\n",
      "3          inn    theatre       bet      miss      dead     scene\n",
      "4        north   redhouse      fair      song        dy      cuse\n",
      "5         cafe     rarely       day      time      buzz   fashion\n",
      "6          bar    musical   perform      city        tv      live\n",
      "7  skaneateles   cortland      tour     watch     actor    friday\n",
      "8        house     hangar     state      band     dance   central\n",
      "9       auburn   landmark   weekend     david    pmbuzz       new\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=6, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_entertainment[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 6)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.40\n",
      "Topic 0: ['upstate', 'man', 'woman', 'western', 'police', 'deer', 'kill', 'charge', 'year', 'arrest']\n",
      "Topic 1: ['west', 'genesee', 'north', 'art', 'new', 'award', 'upstate', 'owner', 'dinner', 'president']\n",
      "Topic 2: ['school', 'state', 'upstate', 'fair', 'new', 'district', 'best', 'day', 'student', 'central']\n",
      "Topic 3: ['lake', 'fish', 'angler', 'river', 'catch', 'salmon', 'football', 'oneida', 'big', 'watch']\n",
      "Topic 4: ['week', 'whats', 'state', 'upstate', 'academy', 'delay', 'store', 'lake', 'fishing', 'eat']\n",
      "Topic 5: ['highschool', 'prom', 'photo', 'house', 'student', 'junior', 'home', 'senior', 'onondaga', 'week']\n",
      "Topic 6: ['fair', 'food', 'review', 'highschool', 'graduation', 'court', 'class', 'chevy', 'king', 'upstate']\n",
      "       Topic #01    Topic #02      Topic #03 Topic #04    Topic #05  \\\n",
      "0           fair         week     highschool   upstate        photo   \n",
      "1           food        house     graduation      best         prom   \n",
      "2         review         home           list       man       junior   \n",
      "3            row         road          class    police   basketball   \n",
      "4     restaurant           st        musical     woman       senior   \n",
      "5       building     colonial  valedictorian    ranked   highschool   \n",
      "6          chevy    cazenovia   salutatorian    report         ball   \n",
      "7         midway         lake        present    charge  skaneateles   \n",
      "8          court      manlius        student      kill    marcellus   \n",
      "9  international  skaneateles         spring      year        video   \n",
      "\n",
      "    Topic #06 Topic #07  \n",
      "0       state    school  \n",
      "1        fair   central  \n",
      "2         day   student  \n",
      "3  basketball       new  \n",
      "4         new  district  \n",
      "5       video      buzz  \n",
      "6         win      year  \n",
      "7       thing   closing  \n",
      "8       cuomo     board  \n",
      "9        best    middle  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=7, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_localnews[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 7)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.11\n",
      "Topic 0: ['football', 'su', 'coach', 'babers', 'dino', 'battle', 'new', 'mets', 'player', 'tyus']\n",
      "Topic 1: ['basketball', 'jim', 'boeheim', 'game', 'orange', 'watch', 'performance', 'coach', 'conference', 'player']\n",
      "Topic 2: ['lacrosse', 'win', 'woman', 'team', 'men', 'world', 'su', 'bowl', 'championship', 'game']\n",
      "Topic 3: ['crunch', 'basketball', 'orange', 'tampa', 'bay', 'football', 'red', 'game', 'ticket', 'player']\n",
      "Topic 4: ['crunch', 'win', 'yankee', 'game', 'recruiting', 'basketball', 'star', 'woman', 'video', 'national']\n",
      "Topic 5: ['channel', 'time', 'live', 'tv', 'basketball', 'stream', 'v', 'game', 'info', 'ncaa']\n",
      "Topic 6: ['football', 'bill', 'buffalo', 'nfl', 'say', 'coach', 'su', 'week', 'receiver', 'offer']\n",
      "Topic 7: ['football', 'rate', 'grade', 'lacrosse', 'team', 'basketball', 'state', 'north', 'win', 'carolina']\n",
      "    Topic #01   Topic #02   Topic #03   Topic #04 Topic #05   Topic #06  \\\n",
      "0  basketball    football    lacrosse      crunch      game        team   \n",
      "1      player          su       woman         new      live        meet   \n",
      "2       woman     recruit         men      goalie        tv         boy   \n",
      "3  tournament      player          su      player      time        girl   \n",
      "4      orange      orange         win     forward   channel      soccer   \n",
      "5        duke     clemson        ncaa       tampa    stream  highschool   \n",
      "6        ncaa        star      player         bay       win          su   \n",
      "7       state  recruiting  tournament  defenseman      info    baseball   \n",
      "8     college    practice        girl         ahl   buffalo      hockey   \n",
      "9         acc  highschool          le       video      mets      league   \n",
      "\n",
      "    Topic #07 Topic #08  \n",
      "0       coach   central  \n",
      "1     boeheim    golfer  \n",
      "2         jim      hole  \n",
      "3         win      golf  \n",
      "4        army       aug  \n",
      "5  conference      meet  \n",
      "6       watch       win  \n",
      "7        mike     state  \n",
      "8         new      girl  \n",
      "9   assistant       boy  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_sports[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.67\n",
      "Topic 0: ['comment', 'school', 'today', 'new', 'weather', 'student', 'work', 'city', 'law', 'turn']\n",
      "Topic 1: ['number', 'worker', 'snow', 'start', 'su', 'know', 'wednesday', 'morning', 'good', 'thursday']\n",
      "Topic 2: ['west', 'genesee', 'girl', 'win', 'section', 'iii', 'basketball', 'class', 'video', 'team']\n",
      "Topic 3: ['county', 'onondaga', 'state', 'plan', 'lake', 'oswego', 'oneida', 'vote', 'union', 'go']\n",
      "Topic 4: ['update', 'win', 'state', 'lottery', 'westhill', 'fatal', 'week', 'identify', 'million', 'championship']\n",
      "Topic 5: ['upstate', 'obits', 'watch', 'whats', 'update', 'center', 'state', 'college', 'national', 'community']\n",
      "Topic 6: ['man', 'police', 'woman', 'charge', 'year', 'crash', 'kill', 'car', 'old', 'accuse']\n",
      "Topic 7: ['say', 'court', 'officer', 'traffic', 'police', 'case', 'son', 'trump', 'chief', 'stop']\n",
      "     Topic #01 Topic #02   Topic #03  Topic #04  Topic #05 Topic #06  \\\n",
      "0       county       man       state      compa        win    school   \n",
      "1     onondaga    charge         boy       join    lottery   central   \n",
      "2     judgment    police        girl    promote     number   student   \n",
      "3   bankruptcy     woman     section       hire    million        su   \n",
      "4       oswego     crash         iii      group    jackpot       day   \n",
      "5  certificate      kill  basketball  associate  wednesday  district   \n",
      "6     business    arrest       class      award  powerball      city   \n",
      "7         lake    accuse  highschool   director    tuesday   college   \n",
      "8      warrant     shoot        team    michael       mega     board   \n",
      "9          tax       car    football     joseph     friday     photo   \n",
      "\n",
      "    Topic #07 Topic #08  \n",
      "0       today      year  \n",
      "1        obit       new  \n",
      "2        work       old  \n",
      "3    graduate    prison  \n",
      "4  highschool  sentence  \n",
      "5    neighbor      open  \n",
      "6      retire   citizen  \n",
      "7       north      home  \n",
      "8     opinion     swear  \n",
      "9   liverpool      life  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_news[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -9.37\n",
      "Topic 0: ['photo', 'prom', 'crash', 'highschool', 'north', 'watch', 'senior', 'dont', 'story', 'upstate']\n",
      "Topic 1: ['live', 'year', 'help', 'man', 'win', 'charge', 'old', 'teen', 'whats', 'family']\n",
      "Topic 2: ['upstate', 'day', 'weekend', 'tv', 'life', 'update', 'video', 'ice', 'let', 'event']\n",
      "Topic 3: ['store', 'new', 'vintage', 'open', 'episode', 'year', 'center', 'desti', 'mall', 'photo']\n",
      "Topic 4: ['traffic', 'buzz', 'video', 'dy', 'time', 'alert', 'su', 'wife', 'daily', 'oz']\n",
      "Topic 5: ['county', 'world', 'onondaga', 'chief', 'video', 'new', 'day', 'music', 'real', 'photo']\n",
      "Topic 6: ['thursday', 'report', 'week', 'series', 'season', 'photo', 'house', 'contest', 'winner', 'race']\n",
      "Topic 7: ['viralvideo', 'home', 'day', 'fall', 'su', 'turn', 'parade', 'student', 'west', 'photo']\n",
      "     Topic #01 Topic #02 Topic #03 Topic #04   Topic #05 Topic #06 Topic #07  \\\n",
      "0         week      hole       ski      home       photo       new   central   \n",
      "1        house       aug    report    parade        prom      year      best   \n",
      "2         dirt      list        th    custom  highschool        su       day   \n",
      "3        super       fun      year   builder     vintage     store        su   \n",
      "4           rd     event        rd     house       video      open    school   \n",
      "5       garden      east    school      mark         day       old   weekend   \n",
      "6  skaneateles      golf     james    martin     contest     video       job   \n",
      "7    cazenovia    course  accident      sale          su      time      food   \n",
      "8     onondaga       big     steve      sell      senior      look      teen   \n",
      "9          win      quot   medical    county        ball     desti      fall   \n",
      "\n",
      "  Topic #08  \n",
      "0     chief  \n",
      "1       win  \n",
      "2       day  \n",
      "3      game  \n",
      "4  national  \n",
      "5     video  \n",
      "6     night  \n",
      "7       fan  \n",
      "8  baseball  \n",
      "9       run  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = list(content_subset_others[\"Headlines\"])\n",
    "corp = [preprocess(line) for line in txt]\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "test_eta_lda('auto', dictionary, ntopics = 8)\n",
    "\n",
    "train_headlines_sentences = [' '.join(text) for text in corp]\n",
    "test_eta_nmf('auto', dictionary, train_headlines_sentences, ntopics = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_opposite = {\n",
    "    'crash':0, 'vehicle':0, 'car':0, 'truck':0, 'unlicensed':0, 'pile':0, 'hit':0, 'driver':0,\n",
    "    'drugs':3, 'heroin':3, 'fentanyl':3, 'street':3,\n",
    "    'shooting':4, 'gun':4, 'shooter':4, 'shot':4, 'accidental':4, 'armed':4, 'gunpoint':4, 'shoot':4, 'homicide':4,\n",
    "    'stabbed':5, 'stealing':5, 'murder':5, 'theft':5, 'victim':5, 'suspect':5, 'fatal':5, 'steal':5, 'stab':5,\n",
    "    'fire':2, 'blaze':2, 'firefighter':2, 'house':2,\n",
    "    'rape':1, 'child':1, 'abuse':1, 'pornography':1, 'teen':1, 'woman':1,\n",
    "}\n",
    "eta = create_eta(apriori_opposite, dictionary, 6)\n",
    "test_eta_lda(eta, dictionary, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
